from audio_augmentor.base import BaseAugmentor
from audio_augmentor.adversarial.rawnet2 import ArtRawnet2
from audio_augmentor.adversarial.assist_ssl import Model as AssistSSL
from audio_augmentor.utils import librosa_to_pydub

from art.attacks.evasion import ProjectedGradientDescent

import numpy as np
SUPPORTED_CM = ["rawnet2", "assist_ssl"]
SUPPORTED_ADV = ["ProjectedGradientDescent"]
class AdversarialNoiseAugmentor(BaseAugmentor):
    """Adversarial noise augmentor.

    This augmentor adds adversarial noise to the input audio.
    The adversarial noise is generated by the Fast Gradient Sign Method (FGSM).

    config:
        model_name: name of the classifier (CM) model. Supported models: ${SUPPORTED_CM}
        model_pretrained: path to the pretrained CM model
        config_path: path to the configuration file of the CM model
        device: device to run the CM model (cpu or cuda)
        adv_method: name of the adversarial method - supported by ART evasion attacks: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks
                current supported methods: ${SUPPORTED_ADV}
        adv_config: configuration of the adversarial method
    """
    def __init__(self, input_path, config):
        super().__init__(input_path, config)
        self.model_name = config["model_name"]
        self.model_pretrained = config["model_pretrained"]
        self.device = config["device"]
        self.adv_method = config["adv_method"]
        
        # load model
        assert self.model_name in SUPPORTED_CM, "model_name must be one of {}".format(SUPPORTED_CM)
        if self.model_name == "rawnet2":
            self.artmodel = ArtRawnet2(config_path = config["config_path"], device = self.device)
            self.artmodel.load_model(self.model_pretrained)
        
        # load adversarial class
        self.adv_class = globals()[self.adv_method](self.artmodel.get_art(), **config["adv_config"])
            
    def load(self):
        super().load()
    
    def transform(self):
        # get classifier_art
        classifier_art = self.artmodel.get_art()
        
        # chunk audio
        chunk_size = len(self.data) // self.artmodel.input_shape[1]
        last_size = len(self.data) % self.artmodel.input_shape[1]
        if chunk_size == 0:
            # no need to split and concat after process
            temp = self.data
            temp = self.artmodel.parse_input(temp)
            temp = self.adv_class.generate(x=temp.cpu().numpy())[0,:]
            # recover to original length
            self.augmented_audio = librosa_to_pydub(temp[:last_size],sr=self.sr)
            
        else:
            adv_res=np.array([], dtype=np.float32)
            for i in chunk_size:
                temp = self.data[i*self.artmodel.input_shape[1]:(i+1)*self.artmodel.input_shape[1]]
                temp = self.artmodel.parse_input(temp)
                temp = self.adv_class.generate(x=temp.cpu().numpy)[0,:]
                adv_res = np.concatenate((adv_res, temp))
            # recover to original length
            self.augmented_audio = librosa_to_pydub(adv_res[:len(self.data + self.artmodel.input_shape[1]) - last_size],sr=self.sr)
        
        
                
            
                
        
    